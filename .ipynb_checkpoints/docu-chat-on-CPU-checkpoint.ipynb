{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4552836a",
   "metadata": {},
   "source": [
    "# Docu-Chat on a CPU\n",
    "\n",
    "This shows how to build a personal assistant which has access to your documents (you provide the folder address) and you can chat with it, on-device, using CPU only. It is completely private, and pretty fast to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6373ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = 'MBZUAI/LaMini-T5-738M'\n",
    "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "device = 'cpu'\n",
    "docs_folder = 'source_documents/'\n",
    "docx_address = 'C:/Users/rezabonyadi/Desktop/test_folder/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a3cd8",
   "metadata": {},
   "source": [
    "# Read docs and embed\n",
    "\n",
    "- First, we read the docx files recursively from the docx_address folder.\n",
    "- We then save them as text files in docs_folder\n",
    "- We finally embed the paragraphs (can be improved significantly)\n",
    "- Save the embeddings in a vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0884e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting content of  C:/Users/rezabonyadi/Desktop/test_folder/creating fictional universe.docx\n",
      "writing:  source_documents/creating fictional universe.txt\n",
      "extracting content of  C:/Users/rezabonyadi/Desktop/test_folder/future of human computer interaction.docx\n",
      "writing:  source_documents/future of human computer interaction.txt\n",
      "extracting content of  C:/Users/rezabonyadi/Desktop/test_folder/natural language programming.docx\n",
      "writing:  source_documents/natural language programming.txt\n",
      "extracting content of  C:/Users/rezabonyadi/Desktop/test_folder/LLMops\\addressing soft challenges llms.docx\n",
      "writing:  source_documents/addressing soft challenges llms.txt\n",
      "extracting content of  C:/Users/rezabonyadi/Desktop/test_folder/LLMops\\operationalizing language models.docx\n",
      "writing:  source_documents/operationalizing language models.txt\n",
      "Done reading!\n",
      "Loading documents from source_documents/\n",
      "Loaded 5 documents from source_documents/\n",
      "Split into 342 chunks of text (max. 500 characters each)\n",
      "Done building db of your documents!\n"
     ]
    }
   ],
   "source": [
    "from utils import build_datasets, prepare_chat_model, settings\n",
    "\n",
    "build_datasets.get_docs(docx_address, docs_folder)\n",
    "print('Done reading!')\n",
    "\n",
    "build_datasets.build_embedding_dataset(docs_folder, embedding_model, device = device)\n",
    "print('Done building db of your documents!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccb62b",
   "metadata": {},
   "source": [
    "# Prepare chat engine and test it\n",
    "- Firs build a retriver (a connection to vector db to retrive relevant contexts using semantic search)\n",
    "- Prepare the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d086bdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriver and Q&A engine ready!\n"
     ]
    }
   ],
   "source": [
    "retriver = prepare_chat_model.get_context_retriver(embedding_model, device='cpu')\n",
    "qa_engine = prepare_chat_model.get_qa_engine(chat_model, retriver, input_max_length=512, \n",
    "                                             pipeline_model=\"text2text-generation\", device=-1)\n",
    "print('Retriver and Q&A engine ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b205aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The challenges with language models include running cost, latency, trust and safety considerations, hallucination, environmental footprints, and privacy challenges.\n"
     ]
    }
   ],
   "source": [
    "def chatbot_response(user_input, qa_engine):\n",
    "    # Put the inference and the LLM stuff here...\n",
    "    query = user_input\n",
    "    res = qa_engine(query)\n",
    "    answer, docs = res['result'], res['source_documents']\n",
    "    print(answer)\n",
    "    all_context = \"\\n\\n\".join(['Context: ' + i.dict()['page_content'] + '\\n \\n Taken from: ' + i.dict()['metadata']['source'] for i in docs])\n",
    "\n",
    "    return answer, all_context\n",
    "\n",
    "user_input = 'What are some challenges with language models?'\n",
    "response, context = chatbot_response(user_input, qa_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e839f5",
   "metadata": {},
   "source": [
    "# More fancy chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8dc267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dd84b4e2de4282bf9c7eaf5791c5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='Hello, how are you?', description='Input:'), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.chatbot_interactive(user_input)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def chatbot_interactive(user_input):\n",
    "    response, context = chatbot_response(user_input, qa_engine)\n",
    "    return response\n",
    "\n",
    "interact(chatbot_interactive, user_input=widgets.Text(value='Hello, how are you?', description='Input:'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
